# configs/training/stylegan_default_training.yaml

# PyTorch Lightning Trainer settings
accelerator: "auto"
devices: "auto" # Or e.g., [0,1] for multi-GPU, or "auto"
max_epochs: -1 # StyleGAN is typically not trained for epochs but for kimg or total steps
# We will use max_steps calculated from total_kimg.
# For example, if total_kimg=1000, dataset_size=450, batch_size=16:
# total_images_to_see = 1000 * 1000 = 1,000,000
# max_steps = total_images_to_see / batch_size = 1,000,000 / 16 = 62500
# This calculation will be done in train.py or the LightningModule.

# Optimizer settings (to be used by the LightningModule)
g_lr: 0.0025
d_lr: 0.0025
adam_betas: [0.0, 0.99]
adam_eps: 1.0e-8

# Regularization (StyleGAN2-ADA specific)
r1_gamma: 10.0 # Default for StyleGAN2. Adjusted based on resolution / dataset.
# For 256x256, often values like 0.5 to 2.0 are used if r1_gamma is scaled by pixels.
# The official repo's loss scales it by 1/pixels, so a base gamma of 10 is common.
# We will follow the official loss implementation.
lazy_regularization: true # If true, R1 and PL are applied less frequently.
d_reg_interval: 16 # How often to apply R1 for D (if lazy_regularization)
g_reg_interval: 4  # How often to apply PL for G (if lazy_regularization)

pl_weight: 2.0     # Path length regularization weight. Set to 0 to disable.
pl_decay: 0.01     # Path length moving average decay.
# pl_batch_shrink: 2 # Factor to shrink batch size for PL calculation if OOM.

# ADA (Adaptive Discriminator Augmentation) settings
ada_target: 0.6      # Target real_signs_estimate for ADA.
ada_interval: 4      # How often to update ADA p (in ticks of G/D steps).
ada_kimg: 100        # ADA p value reaches target value after this many kimg.

# Training duration and logging (StyleGAN terms)
total_kimg: 1000      # Total thousands of real images to show to D.
kimg_per_tick: 4       # How often to print status messages, save snapshots/metrics (in kimg).
                       # This will translate to `log_every_n_steps` and `checkpoint_every_n_train_steps`

# Checkpointing (controlled by kimg_per_tick via Lightning's step-based checkpointing)
checkpoint_every_n_train_steps: null # Set to null to let train.py calculate it, or set a specific step interval.
# checkpoint_save_last: true (Handled by ModelCheckpoint in train.py)

# Logging
log_every_n_steps: 50 # How often to log to TensorBoard etc. (Added this line)

# Precision (example, StyleGAN2-ADA often uses float32 but can be experimented with)
# precision: "bf16-mixed" # or "16-mixed"
precision: "32-true" # Default to full 32-bit precision

# Snapshot settings (for generating image grids during training)
# These will be handled by a callback in the LightningModule
nimg_snapshot: 16 # Number of images in snapshot grid (e.g., 4x4)
snapshot_ticks: 10 # How often to save snapshots (in ticks, where 1 tick = kimg_per_tick)

